<!DOCTYPE html>
<html lang="en">
<head>
<title>activloss</title>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<link rel="stylesheet" href="css/w3.css">
<link rel="stylesheet" href="css/lato.css">
<link rel="stylesheet" href="css/montserrat.css">
<link rel="stylesheet" href="css/font-awesome.css">
<link rel="stylesheet" href="css/prism.css">
<script src="js/prism.js"></script>
<style>
body,h1,h2,h3,h4,h5,h6 {font-family: "Lato", sans-serif}
.w3-bar,h1,button {font-family: "Montserrat", sans-serif}
.fa-anchor,.fa-coffee {font-size:200px}
</style>
</head>
<body>

<!-- Navbar -->
<div class="w3-top">
  <div class="w3-bar w3-grey w3-card w3-left-align w3-large">
    <a href="#" class="w3-bar-item w3-button w3-padding-large w3-white">home</a>
  </div>
</div>

<!-- Header -->
<header class="w3-container w3-blue-grey w3-center" style="padding:128px 16px">
  <h1 class="w3-margin w3-jumbo">CLASSIFICATION DETAILS</h1>
  <p class="w3-xlarge">activation and loss functions for classification</p>
</header>

<!-- First Grid -->
<div class="w3-row-padding w3-padding-64 w3-container">
<div class="w3-content">
<div class="w3-threequarter">

<h1>What activation function is used for object classification?</h1>

<h5 class="w3-padding-32 w3-text-grey">
  Softmax activation.
</h5>

<p>
  For regression problems, we typically use "linear" activation for the output
  neuron, which is equivalent to taking the raw output from the output
  neuron and not applying any nonlinear activation function. So, the
  output of a neural network used for regression is typically a real-valued
  number (ie, a "floating point" number), which can be anywhere between
  -&infin; and +&infin;.
</p>

<p>
  In the case of object classification, we want the neural network to output
  a predicted 'class label' for each data sample. For one-hot-encoded class
  labels, the class label is 'encoded' as a binary vector with dimension
  equal to the number of possible classes. The one-hot-encoded class label
  has zero entries for every class <em>except</em> the class the
  data sample belongs to, which gets an enty of one.
</p>

<p>
  We can get our neural network to output a k-dimensional vector by setting
  the output layer of the network to have k neurons or "units". But, how
  do we get the output layer to produce one-hot-encoded outputs?
</p>

<p>
  Well, in reality, we don't want our netowrk to output one-hot-encoded object
  classification predictions. For <em>labelled</em> data in which we know
  the correct classification with absolute certainty, one-hot encoding makes
  sense. If we know the correct class label, we can set the <em>probability</em>
  of the correct class to 1.0, and we can set the probability of every
  <em>other</em> possible class to 0.0; this is exactly what one-hot
  encoding does!
</p>

<p>
  But, when we are <em>predicting</em> an object's class, we'd prefer the
  predictive model to output a <em>probability distribution</em> over all
  possible classes, where the value assigned to each possible class label
  represents the <em>probability</em> that the data sample came from that
  class, and the sum of the probabilities over <em>all possible</em> classes
  is 1.0. This allows us to assess the <em>confidence</em> of the model in
  its prediction.
</p>

<p>
  Let's look at an example. Let's say we want to classify images as either
  "dog", "cat" or "human". If we assume this ordering, we can one-hot encode
  an image of a dog as the vector (1,0,0), a cat as the vector (0,1,0)
  and an image of a human as (0,0,1). But what if we are certain an image
  is <em>not</em> a human, but it could be either a dog or a cat? We could
  encode this information as the vector (0.5,0.5,0). This is <em>not</em> a
  one-hot encoding! But if we 'read' this vector as a probability distribution,
  it <em>does</em> encode the information that we are <em>certain</em> the
  image is <em>not</em> a human, because the probability of the "human"
  entry is zero. Also, the probability that the image is a "dog" is 0.5,
  and the probability that it is a "cat" is 0.5.
</p>

<p>
  In general, we could encode <em>any</em> ambiguous classification as a
  probability distribution over the three possible classes, in this case.
  For example, if we are 95% certain an image is a human, we could encode
  this information as the classification vector (0.025,0.025,0.95), assuming
  we are equally unsure about whether the image could be a dog or a cat. So
  long as <em>each</em> of the vector entries is &ge;0 and they all sum to
  1.0, the vector is a valid probability distribution over the possible
  classes. These probability distributions are by far the most common way
  to encode the output of neural network classifiers.
</p>

<h5>How do we get a neural network to output a probability distribution?</h5>

<p>
  The "softmax" activation function is a 'special' activation function that
  converts the output of a neural network layer into a probability distribution
  over the number of neurons ("units") in the layer.
</p>

<p>
  Unlike a 'typical' activation function, "softmax" activation is <em>not</em>
  applied <em>independently</em> to each of the neuron's outputs. Rather,
  softmax activation <em>combines</em> the outputs from <em>all</em> of the
  neurons in the layer and converts those outputs into a probability
  distribution (ie, each output is &ge;0, and the sum of all the outputs is
  1.0).
</p>

<p>
  Technically, the softmax activation function is given by the equation:
</p>

<img src="./media/softmax_equation.svg"/>

<p>
  where <em>z</em> are the raw neuron outputs, and there are <em>K</em>
  possible class labels. The important thing to notice is that each neuron's
  activation is divided by the <em>sum</em> of all the activations from all
  the neurons in the layer, effectively <em>normalizing</em> the activations
  to sum to 1.0. Raising <em>e</em> to the power of the activation ensures
  that each of the activations is &ge;0. So, softmax activation produces a
  probability distribution over all output neurons, given the raw neuron
  outputs.
</p>

<h4>The special case of binary classification</h4>

<p>
  When there are only two possible classes, this is called
  "binary classification", and it is often treated a little differently than
  when there are more than two possible classes.
</p>

<p>
  In the case of only two possible classes, if you know the probability that
  an object is in one class, then you <em>automatically</em> know the
  probability that it is in the other class. This makes the probability
  calculation a bit simpler, and it also allows you to make your model
  simpler.
</p>

<p>
  For binary classification problems, we can use a <em>single</em> data
  column to represent the data sample's class. The entry in the data column
  of zero means that the data sample belongs to the first class, and an
  entry of one means the data sample belongs to the other class. For
  example, if we want to classify images as either "cat" or "dog", we can
  encode "cat" as zero and "dog" as one.
</p>

<p>
  So, for binary classification, we can represent an object's class label
  as a vecor (rank-1 tensor) of dimension 1. In this example, the class label
  vector represents the probability that the data sample or object is a
  "dog". And, of course:
</p>

<pre class='language-none'>
Probability("cat") = 1 - Probability("dog")
</pre>

<p>
  Now that we've encoded our binary class label as a 1-dimensional vector,
  we can use a <em>single</em> output neuron to produce our class predictions.
</p>

<p>
  The single output neuron needs to output the probability
  that the data sample is a "dog", in this example. This output must be
  between zero and one (because it's a probability), so linear activation
  is not appropriate.
</p>

<p>
  However, <em>sigmoid</em> activation takes a raw neuron's output and
  transforms it into a value between zero and one using the equation:
</p>

<pre class='language-none'>
sigmoid(z) = 1 / (1 + <em>e</em>^-z)
</pre>

<p>
  where <em>e</em>^-z means 'raise <em>e</em> to the power -z', and z
  is the neuron's raw output. When the neuron's output z is zero,
  sigmoid(z)=0.5. As z gets larger, sigmoid(z) approaches 1.0, and as
  z gets increasingly negative, sigmoid(z) approaches 0.0.
</p>

<p>
  Perfect! That's all we need to do binary classification.
</p>

</div>
<div class="w3-quarter w3-center">
<i class="fa fa-bar-chart-o fa-5x w3-padding-64 w3-text-blue w3-margin-right"></i>
</div>
</div>
</div>

<!-- Second Grid -->
<div class="w3-row-padding w3-light-grey w3-padding-64 w3-container">
<div class="w3-content">
<div class="w3-quarter w3-center">
<i class="fa fa-arrows-h fa-5x w3-padding-64 w3-text-blue w3-margin-right"></i>
</div>

<div class="w3-threequarter">

<h1>What loss function is used for object classification?</h1>

<h5 class="w3-padding-32 w3-text-grey">
  Cross-entropy loss.
</h5>

<p>
  For object classification problems, we want a loss function that encourages
  the model to produce high probabilities for the object's <em>correct</em>
  class and penalizes the model when it produces <em>low</em> probabilities
  for the object's correct class. Obviously, loss functions like mean squared
  error - which are appropriate for regression problems - aren't going to
  work well for classification problems.
</p>

<p>
  By far the most commonly-used loss function for object classification is
  "categorical cross-entropy", which is often also called simply,
  "cross-entropy".
</p>

<p>
  Categorical cross-entropy loss is calculated as:
</p>

<pre class='language-none'>
cross_entropy_loss = -log( probability_of_correct_class )
</pre>

<p>
  In words, the cross-entropy loss is calculated by taking the negative
  logarithm of the model's predicted probability of the <em>correct</em>
  class label.
</p>

<p>
  So, if the correct class label for a data sample is "dog", the cross-entropy
  loss is the negative log of the model's predicted probability that the
  data sample is a "dog".
</p>

<p>
  Looks pretty simple! But does it work?
</p>

<p>
  Let's run through a few examples to check.
</p>

<p>
  First of all, if the model correctly predicts an object's class with
  <em>maximal</em> probability of 1.0, we get:
</p>

<pre class='language-none'>
loss = -log(1.0) = 0.0
</pre>

<p>
  So, that looks good: correctly predicting an object's class with absolute
  certainty results in zero cross-entropy loss.
</p>

<p>
  What if the model predicts the correct class with <em>lower</em> certainty,
  say 0.9?
</p>

<pre class='language-none'>
loss = -log(0.9) = 0.0457
</pre>

<p>
  In this case, the cross-entropy loss goes up, but not by very much. So, the
  model is being <em>penalized</em> for being less certain in its correct
  prediction.
</p>

<p>
  What happens when the model predicts the <em>wrong</em> class?
</p>

<p>
  For the model to predict an incorrect class label, the probability of a
  <em>wrong</em> class label must be <em>higher</em> than the probability of
  the <em>correct</em> class label. In other words, when the model makes a
  <em>wrong</em> prediction, the probability of the <em>correct</em> class
  will be <em>low</em>.
</p>

<p>
  For example:
</p>

<pre class='language-none'>
loss = -log(0.1) = 1.0
</pre>

<p>
  Assuming base-10 logarithms. Cross-entropy loss goes up when the probability
  of the correct class goes down. In a more extreme case:
</p>

<pre class='language-none'>
loss = -log(0.0001) = 4.0
</pre>

<p>
  So, we're going in the 'right' direction; the <em>lower</em> the probability
  of the correct class, the <em>higher</em> the cross-entropy loss.
</p>

<p>
  A potential problem arises if the model assigns a probability of <em>zero</em>
  to the correct class, because the negative logarithm of 0.0 is undefined!
  As the probability of the correct class approaches zero, the negative log of
  that probability approaches +&infin;. We can fix this problem in practice
  by either assigning a <em>maximum</em> cross-entropy loss whenever the
  model's predicted probability of the correct class gets <em>too low</em> or
  by assigning some <em>minimum</em> probability to the correct class in the
  model's output, both of which essentially do the 'same thing'.
</p>


</div>
</div>
</div>


<div class="w3-container w3-black w3-center w3-opacity w3-padding-64">
    <h1 class="w3-margin w3-xlarge">what you have learned == what you can do</h1>
</div>

<!-- Footer -->
<footer class="w3-container w3-padding-64 w3-center w3-opacity">
  <div class="w3-xlarge w3-padding-32">
    <p>end.</p>
 </div>
</footer>

</body>
</html>
